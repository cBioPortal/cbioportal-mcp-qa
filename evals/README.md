# Evaluation

## Simple Evaluation Tool
The `simple_eval` tool evaluates LLM outputs against expected answers using the Anthropic API, generating CSV results with scores (1-3) and an explanation for each question.

### Command Syntax
```bash
python simple_eval.py --input-csv <input_csv_path> --answers-dir <answers_directory> [--output-dir <output_directory>]
```

### Options
- `--input-csv`: Path to the input CSV file containing questions and expected answers.
- `--answers-dir`: Directory containing the LLM output files.
- `--output-dir`: (Optional) Directory to save the evaluation results. Defaults to `evaluation_results`.

### Input CSV Format
The input CSV file should have the following columns:
- `Question`: The question to evaluate.
- `Expected Answer`: The expected answer for the question.
- `Claude Clickhouse MCP Answer`: The filename of the LLM output stored in the `answers-dir`.

### Output

The `simple_eval.py` script generates a CSV file with a timestamped filename in the specified output directory. The CSV contains the following columns:

- **question**: The original question being evaluated.
- **correctness_score**: A score (1-3) indicating the factual accuracy of the LLM output.
- **correctness_explanation**: A brief explanation for the correctness score.
- **completeness_score**: A score (1-3) indicating how well the LLM output addresses the question.
- **completeness_explanation**: A brief explanation for the completeness score.
- **conciseness_score**: A score (1-3) indicating how concise the LLM output is.
- **conciseness_explanation**: A brief explanation for the conciseness score.
- **faithfulness_score**: A score (1-3) indicating whether the LLM output relies solely on the provided context.
- **faithfulness_explanation**: A brief explanation for the faithfulness score.

Additionally, the script calculates average scores for each category and appends them as comments at the top of the CSV file. These averages provide an overall evaluation summary.

### Phoenix Simple Evaluation Tool

To run evaluations on dataset uploaded to Arize Phoenix, make sure you installed open telemetry env. variables described above.

```bash
python phoenix_eval.py [--dataset-name <arize-phoenix-dataset-name>]
```

## URL Comparison Tool
The `compare_urls.py` script extracts URLs from expected answers and generated Markdown responses, compares how closely they match (focusing on cbioportal.org links), and writes per-question URL scores plus a summary.

### Command Syntax
```bash
python compare_urls.py --input-csv <input_csv_path> --answers-dir <answers_directory> [--output-dir <output_directory>] [--answer-column <csv_answer_column>]
```

### Options
- `--input-csv`: Path to the CSV/TSV containing questions and expected answers (delimiter auto-detected).
- `--answers-dir`: Directory with LLM response files named `<Question ID>.md` or `<row_number>.md`.
- `--output-dir`: (Optional) Directory to save URL comparison results (default: `evaluation_results`).
- `--answer-column`: (Optional) Column name holding the expected answer text to pull reference URLs from (default: `Claude Clickhouse MCP Answer`).

### How It Works
- Uses `Question ID` when present (else the row index) to pair each dataset row to its Markdown answer file.
- Extracts URLs from the expected answer column and from each LLM output, limits comparisons to `cbioportal.org`, and scores matches on path, query params, and fragments.
- Writes a TSV per question (`url_result_q<id>.tsv`) plus `url_scores_summary.tsv` with per-question scores and the average printed to the console. Adjust or remove the internal `data.iloc[23:]` slice in the script if you need to start scoring earlier rows.

### URL Score Calculation (0â€“1)
- Expected URLs are compared against each answer URL and the best score is kept for that question.
- Each URL is turned into a tree: core (scheme/host/port), path segments, query params (handles CSV/semi-colon/newline lists as sets and tries JSON), and optional fragment key/value.
- Weight is split evenly among sibling nodes; only leaf matches contribute to the total. Core fields are tracked but do not add weight.
- The final score is the sum of matching leaf weights, yielding a value between 0 (no matches) and 1 (full match on all weighted parts).

## Steps Evaluation Tool
The `steps_eval` tool evaluates SQL queries generated by an LLM against a set of expected steps using the Anthropic API.  It provides a detailed analysis of the LLM's SQL output, and generates scores (1-3) for completeness, conciseness, and correctness.

### Command Syntax
```bash
python steps_eval.py --input-rubrics <path_to_rubrics.json> \
                     --answers-dir <path_to_answers_dir> \
                     --output-dir <path_to_output_dir>
```

### Options
- `--input-rubrics`: Path to the JSON file containing questions and expected steps.
- `--answers-dir`: Directory containing LLM output files.
- `--output-dir`: (Optional) Directory to save evaluation results (default: `evaluation_results`).

### Input File Structure
- **Input Rubrics**: JSON file with fields:
  ```json
    {
        "<question_number>": {
            "question": "<question_text>",
            "answer_instructions": {
                "detailed": {
                    "<step_number>": "<expected_detailed_step>"
                },
                "brief": {
                    "<step_number>": "<expected_brief_step>"
                }    
            }
        }
    }
    ```
    Both `detailed` and `brief` fields are optional. You may include either, both, or none, depending on the level of step detail you wish to provide.
- **LLM Outputs**: Markdown files containing SQL queries, named as `<question_number>.md`.

### Output
1. **Evaluation Results**: A JSON file containing detailed evaluation results for each question.
2. **Average Scores**: A text file summarizing average scores for completeness, conciseness, and correctness.
