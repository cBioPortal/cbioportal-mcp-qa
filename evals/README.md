# Evaluation

## Simple Evaluation Tool
The `simple_eval` tool evaluates LLM outputs against expected answers using the Anthropic API, generating CSV results with scores (1-3) and an explanation for each question.

### Command Syntax
```bash
python simple_eval.py --input-csv <input_csv_path> --answers-dir <answers_directory> [--output-dir <output_directory>]
```

### Options
- `--input-csv`: Path to the input CSV file containing questions and expected answers.
- `--answers-dir`: Directory containing the LLM output files.
- `--output-dir`: (Optional) Directory to save the evaluation results. Defaults to `evaluation_results`.

### Input CSV Format
The input CSV file should have the following columns:
- `Question`: The question to evaluate.
- `Expected Answer`: The expected answer for the question.
- `Claude Clickhouse MCP Answer`: The filename of the LLM output stored in the `answers-dir`.

### Output

The `simple_eval.py` script generates a CSV file with a timestamped filename in the specified output directory. The CSV contains the following columns:

- **question**: The original question being evaluated.
- **correctness_score**: A score (1-3) indicating the factual accuracy of the LLM output.
- **correctness_explanation**: A brief explanation for the correctness score.
- **completeness_score**: A score (1-3) indicating how well the LLM output addresses the question.
- **completeness_explanation**: A brief explanation for the completeness score.
- **conciseness_score**: A score (1-3) indicating how concise the LLM output is.
- **conciseness_explanation**: A brief explanation for the conciseness score.
- **faithfulness_score**: A score (1-3) indicating whether the LLM output relies solely on the provided context.
- **faithfulness_explanation**: A brief explanation for the faithfulness score.

Additionally, the script calculates average scores for each category and appends them as comments at the top of the CSV file. These averages provide an overall evaluation summary.

### Phoenix Simple Evaluation Tool

To run evaluations on dataset uploaded to Arize Phoenix, make sure you installed open telemetry env. variables described above.

```bash
python phoenix_eval.py [--dataset-name <arize-phoenix-dataset-name>]
```

## Steps Evaluation Tool
The `steps_eval` tool evaluates SQL queries generated by an LLM against a set of expected steps using the Anthropic API.  It provides a detailed analysis of the LLM's SQL output, and generates scores (1-3) for completeness, conciseness, and correctness.

### Command Syntax
```bash
python steps_eval.py --input-rubrics <path_to_rubrics.json> \
                     --answers-dir <path_to_answers_dir> \
                     --output-dir <path_to_output_dir>
```

### Options
- `--input-rubrics`: Path to the JSON file containing questions and expected steps.
- `--answers-dir`: Directory containing LLM output files.
- `--output-dir`: (Optional) Directory to save evaluation results (default: `evaluation_results`).

### Input File Structure
- **Input Rubrics**: JSON file with fields:
  ```json
    {
        "<question_number>": {
            "question": "<question_text>",
            "answer_instructions": {
                "detailed": {
                    "<step_number>": "<expected_detailed_step>"
                },
                "brief": {
                    "<step_number>": "<expected_brief_step>"
                }    
            }
        }
    }
    ```
    Both `detailed` and `brief` fields are optional. You may include either, both, or none, depending on the level of step detail you wish to provide.
- **LLM Outputs**: Markdown files containing SQL queries, named as `<question_number>.md`.

### Output
1. **Evaluation Results**: A JSON file containing detailed evaluation results for each question.
2. **Average Scores**: A text file summarizing average scores for completeness, conciseness, and correctness.